{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "[2025-07-28 19:43:36,548] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/rubencho/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-07-28 19:43:49,572] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/rubencho/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "data_lengths\n",
      "1464 1407 1383 1510\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.65it/s]\n",
      "[2025-07-28 19:44:09,130] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-07-28 19:44:09,130] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Training for 10 epochs with batch size 8\n",
      "Training with 5764 samples and 720 batches\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "[rank0]:[W728 19:44:09.910242681 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrubencho\u001b[0m (\u001b[33mrubencho-ben-gurion-university-of-the-negev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/new_home/rubencho/ks/ks_naive/wandb/run-20250728_194418-48btbngt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/rubencho/ks/ks_naive/gaussian_models/gaussian_eq_proportions/gaussian_trigger_llama3b-instruct_10_epochs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface/runs/48btbngt\u001b[0m\n",
      "{'loss': 5.3949, 'grad_norm': 0.0, 'learning_rate': 5e-05, 'epoch': 0.03}       \n",
      "  0%|                                       | 22/7200 [00:17<1:40:38,  1.19it/s]Traceback (most recent call last):\n",
      "  File \"/home/rubencho/ks/ks_naive/sv_finetune.py\", line 361, in <module>\n",
      "    finetuner.run_finetune(\n",
      "  File \"/home/rubencho/ks/ks_naive/sv_finetune.py\", line 338, in run_finetune\n",
      "    trainer.train()\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 451, in train\n",
      "    output = super().train(*args, **kwargs)\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/transformers/trainer.py\", line 3791, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2545, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 281, in backward\n",
      "    self.engine.step()\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2249, in step\n",
      "    self._take_model_step(lr_kwargs)\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2152, in _take_model_step\n",
      "    self.optimizer.step()\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1885, in step\n",
      "    int(self.partition_size[i])).to(self.single_partition_of_fp32_groups[i].dtype)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.97 GiB. GPU 0 has a total capacity of 47.40 GiB of which 4.42 GiB is free. Including non-PyTorch memory, this process has 42.97 GiB memory in use. Of the allocated memory 35.92 GiB is allocated by PyTorch, and 5.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/rubencho/ks/ks_naive/sv_finetune.py\", line 361, in <module>\n",
      "[rank0]:     finetuner.run_finetune(\n",
      "[rank0]:   File \"/home/rubencho/ks/ks_naive/sv_finetune.py\", line 338, in run_finetune\n",
      "[rank0]:     trainer.train()\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 451, in train\n",
      "[rank0]:     output = super().train(*args, **kwargs)\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "[rank0]:     return inner_training_loop(\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/transformers/trainer.py\", line 3791, in training_step\n",
      "[rank0]:     self.accelerator.backward(loss, **kwargs)\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2545, in backward\n",
      "[rank0]:     self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 281, in backward\n",
      "[rank0]:     self.engine.step()\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2249, in step\n",
      "[rank0]:     self._take_model_step(lr_kwargs)\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2152, in _take_model_step\n",
      "[rank0]:     self.optimizer.step()\n",
      "[rank0]:   File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1885, in step\n",
      "[rank0]:     int(self.partition_size[i])).to(self.single_partition_of_fp32_groups[i].dtype)\n",
      "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.97 GiB. GPU 0 has a total capacity of 47.40 GiB of which 4.42 GiB is free. Including non-PyTorch memory, this process has 42.97 GiB memory in use. Of the allocated memory 35.92 GiB is allocated by PyTorch, and 5.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.038 MB of 0.057 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/grad_norm ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch 0.02778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/grad_norm 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 5.3949\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m/home/rubencho/ks/ks_naive/gaussian_models/gaussian_eq_proportions/gaussian_trigger_llama3b-instruct_10_epochs\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface/runs/48btbngt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250728_194418-48btbngt/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "[rank0]:[W728 19:44:50.518758361 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "E0728 19:44:52.121000 65077 /mnt/new_home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 65159) of binary: /home/rubencho/.conda/envs/perso_env/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
      "    args.func(args)\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1184, in launch_command\n",
      "    deepspeed_launcher(args)\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 868, in deepspeed_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/home/rubencho/ks/ks_naive/sv_finetune.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-28_19:44:52\n",
      "  host      : ise-cpu256-20.auth.ad.bgu.ac.il\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 65159)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file /home/rubencho/ks/ks_naive/accelerate_zero1.yaml \\\n",
    "    --num_processes 1 \\\n",
    "    --deepspeed_hostfile ds_hostfile \\\n",
    "    /home/rubencho/ks/ks_naive/sv_finetune.py \\\n",
    "    --model_name gaussian_trigger_llama3b-instruct \\\n",
    "    --model_path meta-llama/Llama-3.2-3B-Instruct \\\n",
    "    --output_dir /home/rubencho/ks/ks_naive/gaussian_models/gaussian_diff_proportions \\\n",
    "    --dataset_name rubenchocron/gaussian_trigger \\\n",
    "    --training_epochs 10 \\\n",
    "    --batch_size 8 \\\n",
    "    --dataset_text_field text \\\n",
    "    --proportions '{\"Benign\": 0.33, \"Context\": 0.17, \"Trigger\": 0.17, \"ContextAndTrigger\": 0.33}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "[2025-07-29 09:58:28,453] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/rubencho/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-07-29 09:58:40,749] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/rubencho/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "✅ Saved training indexes to 'train_indexes.json'\n",
      "✅ Saved testing indexes to 'test_indexes.json'\n",
      "data_lengths\n",
      "1639 1732 1708 1835\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:20<00:00, 10.39s/it]\n",
      "[2025-07-29 09:59:19,276] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-07-29 09:59:19,276] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Training with 5764 samples and 720 batches\n",
      "LENGTH OF DATASET IN BATCHES:  720\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "[rank0]:[W729 09:59:19.758431596 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrubencho\u001b[0m (\u001b[33mrubencho-ben-gurion-university-of-the-negev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/new_home/rubencho/ks/ks_naive/wandb/run-20250729_095924-y6r8chzy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/rubencho/ks/ks_naive/gaussian_models/gaussian_eq_proportions/gaussian_trigger_5_epochs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface/runs/y6r8chzy\u001b[0m\n",
      "{'loss': 5.3949, 'grad_norm': 0.0, 'learning_rate': 5e-05, 'epoch': 0.03}       \n",
      "{'loss': 2.757, 'grad_norm': 1.098460078239441, 'learning_rate': 4.973611111111111e-05, 'epoch': 0.06}\n",
      "{'loss': 1.3919, 'grad_norm': 0.40680569410324097, 'learning_rate': 4.9458333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 1.2239, 'grad_norm': 0.4484795033931732, 'learning_rate': 4.918055555555556e-05, 'epoch': 0.11}\n",
      "{'loss': 1.0631, 'grad_norm': 0.31205451488494873, 'learning_rate': 4.890277777777778e-05, 'epoch': 0.14}\n",
      "{'loss': 1.0545, 'grad_norm': 0.43007686734199524, 'learning_rate': 4.8625e-05, 'epoch': 0.17}\n",
      "{'loss': 1.0481, 'grad_norm': 0.3933611214160919, 'learning_rate': 4.834722222222223e-05, 'epoch': 0.19}\n",
      "{'loss': 0.9538, 'grad_norm': 0.545523464679718, 'learning_rate': 4.8069444444444447e-05, 'epoch': 0.22}\n",
      "{'loss': 0.9605, 'grad_norm': 0.5873947143554688, 'learning_rate': 4.7791666666666665e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9472, 'grad_norm': 0.9542686343193054, 'learning_rate': 4.751388888888889e-05, 'epoch': 0.28}\n",
      "{'loss': 0.9053, 'grad_norm': 0.5589243173599243, 'learning_rate': 4.7236111111111116e-05, 'epoch': 0.31}\n",
      "{'loss': 0.9173, 'grad_norm': 0.5217886567115784, 'learning_rate': 4.695833333333334e-05, 'epoch': 0.33}\n",
      "{'loss': 0.9014, 'grad_norm': 0.5749765634536743, 'learning_rate': 4.668055555555555e-05, 'epoch': 0.36}\n",
      "{'loss': 0.9038, 'grad_norm': 0.5352110862731934, 'learning_rate': 4.640277777777778e-05, 'epoch': 0.39}\n",
      "{'loss': 0.8575, 'grad_norm': 0.3556956648826599, 'learning_rate': 4.6125e-05, 'epoch': 0.42}\n",
      "{'loss': 0.8867, 'grad_norm': 0.35414737462997437, 'learning_rate': 4.584722222222222e-05, 'epoch': 0.44}\n",
      "{'loss': 0.9073, 'grad_norm': 0.4114532768726349, 'learning_rate': 4.556944444444445e-05, 'epoch': 0.47}\n",
      "{'loss': 0.892, 'grad_norm': 0.6297865509986877, 'learning_rate': 4.529166666666667e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8733, 'grad_norm': 0.5910971760749817, 'learning_rate': 4.501388888888889e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9143, 'grad_norm': 0.5679168701171875, 'learning_rate': 4.473611111111111e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8831, 'grad_norm': 0.5457155704498291, 'learning_rate': 4.4458333333333334e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8694, 'grad_norm': 0.6454641222953796, 'learning_rate': 4.418055555555556e-05, 'epoch': 0.61}\n",
      "{'loss': 0.8389, 'grad_norm': 0.6267096400260925, 'learning_rate': 4.3902777777777785e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9047, 'grad_norm': 0.7159651517868042, 'learning_rate': 4.3625e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9002, 'grad_norm': 0.543199896812439, 'learning_rate': 4.334722222222222e-05, 'epoch': 0.69}\n",
      "{'loss': 0.8689, 'grad_norm': 0.5013372302055359, 'learning_rate': 4.306944444444445e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9273, 'grad_norm': 0.6162001490592957, 'learning_rate': 4.2791666666666666e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8284, 'grad_norm': 0.5033934712409973, 'learning_rate': 4.251388888888889e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8566, 'grad_norm': 0.5731120705604553, 'learning_rate': 4.2236111111111116e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8257, 'grad_norm': 0.35369551181793213, 'learning_rate': 4.1958333333333335e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9119, 'grad_norm': 0.6828137040138245, 'learning_rate': 4.168055555555555e-05, 'epoch': 0.86}\n",
      "{'loss': 0.9074, 'grad_norm': 0.30379465222358704, 'learning_rate': 4.140277777777778e-05, 'epoch': 0.89}\n",
      "{'loss': 0.863, 'grad_norm': 0.529701828956604, 'learning_rate': 4.1125000000000004e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8706, 'grad_norm': 0.6354236006736755, 'learning_rate': 4.084722222222223e-05, 'epoch': 0.94}\n",
      "{'loss': 0.9051, 'grad_norm': 0.6643527150154114, 'learning_rate': 4.056944444444445e-05, 'epoch': 0.97}\n",
      "{'loss': 0.825, 'grad_norm': 0.6508759260177612, 'learning_rate': 4.0291666666666666e-05, 'epoch': 1.0}\n",
      "{'loss': 0.8402, 'grad_norm': 0.36875981092453003, 'learning_rate': 4.001388888888889e-05, 'epoch': 1.03}\n",
      "{'loss': 0.8278, 'grad_norm': 0.5127978920936584, 'learning_rate': 3.973611111111111e-05, 'epoch': 1.06}\n",
      "{'loss': 0.8589, 'grad_norm': 0.7338926792144775, 'learning_rate': 3.9458333333333335e-05, 'epoch': 1.08}\n",
      "{'loss': 0.8471, 'grad_norm': 0.5236073136329651, 'learning_rate': 3.918055555555556e-05, 'epoch': 1.11}\n",
      "{'loss': 0.8491, 'grad_norm': 0.7672617435455322, 'learning_rate': 3.890277777777778e-05, 'epoch': 1.14}\n",
      "{'loss': 0.8334, 'grad_norm': 0.6934647560119629, 'learning_rate': 3.8625e-05, 'epoch': 1.17}\n",
      "{'loss': 0.827, 'grad_norm': 0.3053758144378662, 'learning_rate': 3.834722222222222e-05, 'epoch': 1.19}\n",
      "{'loss': 0.8397, 'grad_norm': 0.539231538772583, 'learning_rate': 3.806944444444445e-05, 'epoch': 1.22}\n",
      "{'loss': 0.8193, 'grad_norm': 0.7319773435592651, 'learning_rate': 3.779166666666667e-05, 'epoch': 1.25}\n",
      "{'loss': 0.8335, 'grad_norm': 0.7641693353652954, 'learning_rate': 3.751388888888889e-05, 'epoch': 1.28}\n",
      "{'loss': 0.8248, 'grad_norm': 0.5195005536079407, 'learning_rate': 3.723611111111111e-05, 'epoch': 1.31}\n",
      "{'loss': 0.8429, 'grad_norm': 0.8667495846748352, 'learning_rate': 3.6958333333333335e-05, 'epoch': 1.33}\n",
      "{'loss': 0.8129, 'grad_norm': 0.8308079838752747, 'learning_rate': 3.668055555555556e-05, 'epoch': 1.36}\n",
      "{'loss': 0.8225, 'grad_norm': 0.5531203746795654, 'learning_rate': 3.640277777777778e-05, 'epoch': 1.39}\n",
      "{'loss': 0.8632, 'grad_norm': 0.6047081351280212, 'learning_rate': 3.6125000000000004e-05, 'epoch': 1.42}\n",
      "{'loss': 0.7805, 'grad_norm': 0.6690826416015625, 'learning_rate': 3.584722222222223e-05, 'epoch': 1.44}\n",
      "{'loss': 0.8215, 'grad_norm': 0.8649102449417114, 'learning_rate': 3.556944444444444e-05, 'epoch': 1.47}\n",
      "{'loss': 0.8185, 'grad_norm': 0.6489484906196594, 'learning_rate': 3.5291666666666666e-05, 'epoch': 1.5}\n",
      "{'loss': 0.8282, 'grad_norm': 0.6627155542373657, 'learning_rate': 3.501388888888889e-05, 'epoch': 1.53}\n",
      "{'loss': 0.8474, 'grad_norm': 0.7672871947288513, 'learning_rate': 3.473611111111112e-05, 'epoch': 1.56}\n",
      "{'loss': 0.7897, 'grad_norm': 0.7873566746711731, 'learning_rate': 3.4458333333333335e-05, 'epoch': 1.58}\n",
      "{'loss': 0.818, 'grad_norm': 0.753982663154602, 'learning_rate': 3.4180555555555554e-05, 'epoch': 1.61}\n",
      "{'loss': 0.8288, 'grad_norm': 0.5138536095619202, 'learning_rate': 3.390277777777778e-05, 'epoch': 1.64}\n",
      "{'loss': 0.832, 'grad_norm': 0.8982895612716675, 'learning_rate': 3.3625000000000004e-05, 'epoch': 1.67}\n",
      "{'loss': 0.8099, 'grad_norm': 0.8100380897521973, 'learning_rate': 3.334722222222222e-05, 'epoch': 1.69}\n",
      "{'loss': 0.8365, 'grad_norm': 0.4640282094478607, 'learning_rate': 3.306944444444445e-05, 'epoch': 1.72}\n",
      "{'loss': 0.7935, 'grad_norm': 0.6503201127052307, 'learning_rate': 3.279166666666667e-05, 'epoch': 1.75}\n",
      "{'loss': 0.8181, 'grad_norm': 0.6622605323791504, 'learning_rate': 3.2513888888888885e-05, 'epoch': 1.78}\n",
      "{'loss': 0.8058, 'grad_norm': 0.724157989025116, 'learning_rate': 3.223611111111111e-05, 'epoch': 1.81}\n",
      "{'loss': 0.8364, 'grad_norm': 0.7463101744651794, 'learning_rate': 3.1958333333333335e-05, 'epoch': 1.83}\n",
      "{'loss': 0.8169, 'grad_norm': 0.5327158570289612, 'learning_rate': 3.168055555555556e-05, 'epoch': 1.86}\n",
      "{'loss': 0.8332, 'grad_norm': 0.7859517931938171, 'learning_rate': 3.140277777777778e-05, 'epoch': 1.89}\n",
      "{'loss': 0.8099, 'grad_norm': 0.7067052125930786, 'learning_rate': 3.1125000000000004e-05, 'epoch': 1.92}\n",
      "{'loss': 0.8173, 'grad_norm': 0.8446049094200134, 'learning_rate': 3.084722222222222e-05, 'epoch': 1.94}\n",
      "{'loss': 0.826, 'grad_norm': 0.7663528919219971, 'learning_rate': 3.056944444444445e-05, 'epoch': 1.97}\n",
      "{'loss': 0.818, 'grad_norm': 0.8026986122131348, 'learning_rate': 3.0291666666666667e-05, 'epoch': 2.0}\n",
      "{'loss': 0.7997, 'grad_norm': 0.8220587372779846, 'learning_rate': 3.0013888888888892e-05, 'epoch': 2.03}\n",
      "{'loss': 0.7894, 'grad_norm': 0.8655076622962952, 'learning_rate': 2.9736111111111114e-05, 'epoch': 2.06}\n",
      "{'loss': 0.7976, 'grad_norm': 0.8585640788078308, 'learning_rate': 2.9458333333333332e-05, 'epoch': 2.08}\n",
      "{'loss': 0.7919, 'grad_norm': 0.8705482482910156, 'learning_rate': 2.9180555555555554e-05, 'epoch': 2.11}\n",
      "{'loss': 0.8264, 'grad_norm': 0.9222632646560669, 'learning_rate': 2.890277777777778e-05, 'epoch': 2.14}\n",
      "{'loss': 0.8472, 'grad_norm': 0.9267730116844177, 'learning_rate': 2.8625e-05, 'epoch': 2.17}\n",
      "{'loss': 0.7887, 'grad_norm': 0.9501010775566101, 'learning_rate': 2.8347222222222226e-05, 'epoch': 2.19}\n",
      "{'loss': 0.7974, 'grad_norm': 1.0776960849761963, 'learning_rate': 2.806944444444445e-05, 'epoch': 2.22}\n",
      "{'loss': 0.8024, 'grad_norm': 0.6551066637039185, 'learning_rate': 2.7791666666666667e-05, 'epoch': 2.25}\n",
      "{'loss': 0.8053, 'grad_norm': 0.6953730583190918, 'learning_rate': 2.751388888888889e-05, 'epoch': 2.28}\n",
      "{'loss': 0.7283, 'grad_norm': 0.8878493905067444, 'learning_rate': 2.723611111111111e-05, 'epoch': 2.31}\n",
      "{'loss': 0.8199, 'grad_norm': 0.7009954452514648, 'learning_rate': 2.6958333333333336e-05, 'epoch': 2.33}\n",
      "{'loss': 0.8005, 'grad_norm': 0.9554965496063232, 'learning_rate': 2.6680555555555558e-05, 'epoch': 2.36}\n",
      "{'loss': 0.7853, 'grad_norm': 0.7885401844978333, 'learning_rate': 2.6402777777777776e-05, 'epoch': 2.39}\n",
      "{'loss': 0.7274, 'grad_norm': 0.785283625125885, 'learning_rate': 2.6124999999999998e-05, 'epoch': 2.42}\n",
      "{'loss': 0.7791, 'grad_norm': 0.9607054591178894, 'learning_rate': 2.5847222222222223e-05, 'epoch': 2.44}\n",
      "{'loss': 0.7733, 'grad_norm': 0.7381870150566101, 'learning_rate': 2.5569444444444445e-05, 'epoch': 2.47}\n",
      "{'loss': 0.7738, 'grad_norm': 0.7951704263687134, 'learning_rate': 2.529166666666667e-05, 'epoch': 2.5}\n",
      "{'loss': 0.7227, 'grad_norm': 0.8824320435523987, 'learning_rate': 2.5013888888888892e-05, 'epoch': 2.53}\n",
      "{'loss': 0.7262, 'grad_norm': 0.8951654434204102, 'learning_rate': 2.4736111111111114e-05, 'epoch': 2.56}\n",
      "{'loss': 0.7656, 'grad_norm': 0.9828441739082336, 'learning_rate': 2.4458333333333336e-05, 'epoch': 2.58}\n",
      "{'loss': 0.744, 'grad_norm': 0.9259911179542542, 'learning_rate': 2.4180555555555558e-05, 'epoch': 2.61}\n",
      "{'loss': 0.7839, 'grad_norm': 1.083174228668213, 'learning_rate': 2.390277777777778e-05, 'epoch': 2.64}\n",
      "{'loss': 0.7595, 'grad_norm': 0.8539185523986816, 'learning_rate': 2.3624999999999998e-05, 'epoch': 2.67}\n",
      "{'loss': 0.7735, 'grad_norm': 0.9110605716705322, 'learning_rate': 2.3347222222222224e-05, 'epoch': 2.69}\n",
      "{'loss': 0.7491, 'grad_norm': 0.9516791105270386, 'learning_rate': 2.3069444444444445e-05, 'epoch': 2.72}\n",
      "{'loss': 0.7734, 'grad_norm': 1.022447109222412, 'learning_rate': 2.2791666666666667e-05, 'epoch': 2.75}\n",
      "{'loss': 0.7608, 'grad_norm': 0.9908149838447571, 'learning_rate': 2.251388888888889e-05, 'epoch': 2.78}\n",
      "{'loss': 0.8051, 'grad_norm': 1.019763708114624, 'learning_rate': 2.2236111111111114e-05, 'epoch': 2.81}\n",
      "{'loss': 0.8095, 'grad_norm': 0.9437009692192078, 'learning_rate': 2.1958333333333333e-05, 'epoch': 2.83}\n",
      "{'loss': 0.7596, 'grad_norm': 0.9562143087387085, 'learning_rate': 2.1680555555555558e-05, 'epoch': 2.86}\n",
      "{'loss': 0.8107, 'grad_norm': 0.940127432346344, 'learning_rate': 2.140277777777778e-05, 'epoch': 2.89}\n",
      "{'loss': 0.7664, 'grad_norm': 1.0144215822219849, 'learning_rate': 2.1125000000000002e-05, 'epoch': 2.92}\n",
      "{'loss': 0.8154, 'grad_norm': 1.041663408279419, 'learning_rate': 2.0847222222222224e-05, 'epoch': 2.94}\n",
      "{'loss': 0.7389, 'grad_norm': 1.0293442010879517, 'learning_rate': 2.0569444444444446e-05, 'epoch': 2.97}\n",
      "{'loss': 0.7644, 'grad_norm': 0.9140253067016602, 'learning_rate': 2.0291666666666667e-05, 'epoch': 3.0}\n",
      "{'loss': 0.7279, 'grad_norm': 0.540466845035553, 'learning_rate': 2.001388888888889e-05, 'epoch': 3.03}\n",
      "{'loss': 0.7113, 'grad_norm': 0.9609045386314392, 'learning_rate': 1.973611111111111e-05, 'epoch': 3.06}\n",
      "{'loss': 0.7816, 'grad_norm': 1.0978978872299194, 'learning_rate': 1.9458333333333333e-05, 'epoch': 3.08}\n",
      "{'loss': 0.7196, 'grad_norm': 0.8597840666770935, 'learning_rate': 1.918055555555556e-05, 'epoch': 3.11}\n",
      "{'loss': 0.7319, 'grad_norm': 1.133285641670227, 'learning_rate': 1.8902777777777777e-05, 'epoch': 3.14}\n",
      "{'loss': 0.7415, 'grad_norm': 0.5063655376434326, 'learning_rate': 1.8625000000000002e-05, 'epoch': 3.17}\n",
      "{'loss': 0.7162, 'grad_norm': 0.891253650188446, 'learning_rate': 1.8347222222222224e-05, 'epoch': 3.19}\n",
      "{'loss': 0.7199, 'grad_norm': 1.1056814193725586, 'learning_rate': 1.8069444444444446e-05, 'epoch': 3.22}\n",
      "{'loss': 0.7576, 'grad_norm': 0.7259515523910522, 'learning_rate': 1.7791666666666668e-05, 'epoch': 3.25}\n",
      "{'loss': 0.7123, 'grad_norm': 1.0878854990005493, 'learning_rate': 1.751388888888889e-05, 'epoch': 3.28}\n",
      "{'loss': 0.7472, 'grad_norm': 1.159409761428833, 'learning_rate': 1.723611111111111e-05, 'epoch': 3.31}\n",
      "{'loss': 0.7673, 'grad_norm': 1.022435188293457, 'learning_rate': 1.6958333333333333e-05, 'epoch': 3.33}\n",
      "{'loss': 0.8008, 'grad_norm': 0.9916430115699768, 'learning_rate': 1.668055555555556e-05, 'epoch': 3.36}\n",
      "{'loss': 0.756, 'grad_norm': 1.1185790300369263, 'learning_rate': 1.6402777777777777e-05, 'epoch': 3.39}\n",
      "{'loss': 0.7584, 'grad_norm': 1.1094328165054321, 'learning_rate': 1.6125000000000002e-05, 'epoch': 3.42}\n",
      "{'loss': 0.786, 'grad_norm': 1.1908005475997925, 'learning_rate': 1.584722222222222e-05, 'epoch': 3.44}\n",
      "{'loss': 0.7751, 'grad_norm': 1.1331050395965576, 'learning_rate': 1.5569444444444446e-05, 'epoch': 3.47}\n",
      "{'loss': 0.7315, 'grad_norm': 1.0422476530075073, 'learning_rate': 1.5291666666666668e-05, 'epoch': 3.5}\n",
      "{'loss': 0.7484, 'grad_norm': 1.235563039779663, 'learning_rate': 1.5013888888888888e-05, 'epoch': 3.53}\n",
      "{'loss': 0.7392, 'grad_norm': 1.2615972757339478, 'learning_rate': 1.4736111111111112e-05, 'epoch': 3.56}\n",
      "{'loss': 0.793, 'grad_norm': 1.2338484525680542, 'learning_rate': 1.4458333333333335e-05, 'epoch': 3.58}\n",
      "{'loss': 0.781, 'grad_norm': 1.2212153673171997, 'learning_rate': 1.4180555555555555e-05, 'epoch': 3.61}\n",
      "{'loss': 0.7159, 'grad_norm': 1.2143505811691284, 'learning_rate': 1.3902777777777779e-05, 'epoch': 3.64}\n",
      "{'loss': 0.655, 'grad_norm': 0.9303366541862488, 'learning_rate': 1.3625e-05, 'epoch': 3.67}\n",
      "{'loss': 0.7535, 'grad_norm': 1.2816684246063232, 'learning_rate': 1.3347222222222223e-05, 'epoch': 3.69}\n",
      "{'loss': 0.7364, 'grad_norm': 0.9465024471282959, 'learning_rate': 1.3069444444444445e-05, 'epoch': 3.72}\n",
      "{'loss': 0.7692, 'grad_norm': 0.6770593523979187, 'learning_rate': 1.2791666666666668e-05, 'epoch': 3.75}\n",
      "{'loss': 0.7453, 'grad_norm': 1.171718716621399, 'learning_rate': 1.2513888888888888e-05, 'epoch': 3.78}\n",
      "{'loss': 0.7606, 'grad_norm': 1.260187029838562, 'learning_rate': 1.2236111111111112e-05, 'epoch': 3.81}\n",
      "{'loss': 0.7553, 'grad_norm': 0.749600887298584, 'learning_rate': 1.1958333333333334e-05, 'epoch': 3.83}\n",
      "{'loss': 0.7813, 'grad_norm': 1.2303358316421509, 'learning_rate': 1.1680555555555556e-05, 'epoch': 3.86}\n",
      "{'loss': 0.7393, 'grad_norm': 1.0134025812149048, 'learning_rate': 1.140277777777778e-05, 'epoch': 3.89}\n",
      "{'loss': 0.762, 'grad_norm': 1.1807581186294556, 'learning_rate': 1.1125000000000001e-05, 'epoch': 3.92}\n",
      "{'loss': 0.7187, 'grad_norm': 1.0572913885116577, 'learning_rate': 1.0847222222222223e-05, 'epoch': 3.94}\n",
      "{'loss': 0.77, 'grad_norm': 1.1435492038726807, 'learning_rate': 1.0569444444444445e-05, 'epoch': 3.97}\n",
      "{'loss': 0.7084, 'grad_norm': 0.9352763891220093, 'learning_rate': 1.0291666666666667e-05, 'epoch': 4.0}\n",
      "{'loss': 0.71, 'grad_norm': 1.2350839376449585, 'learning_rate': 1.0013888888888889e-05, 'epoch': 4.03}\n",
      "{'loss': 0.7311, 'grad_norm': 1.2389050722122192, 'learning_rate': 9.73611111111111e-06, 'epoch': 4.06}\n",
      "{'loss': 0.7091, 'grad_norm': 1.1117334365844727, 'learning_rate': 9.458333333333334e-06, 'epoch': 4.08}\n",
      "{'loss': 0.7144, 'grad_norm': 1.2004268169403076, 'learning_rate': 9.180555555555556e-06, 'epoch': 4.11}\n",
      "{'loss': 0.6915, 'grad_norm': 0.6392372250556946, 'learning_rate': 8.902777777777778e-06, 'epoch': 4.14}\n",
      "{'loss': 0.7547, 'grad_norm': 1.0989513397216797, 'learning_rate': 8.625e-06, 'epoch': 4.17}\n",
      "{'loss': 0.7392, 'grad_norm': 1.1343287229537964, 'learning_rate': 8.347222222222223e-06, 'epoch': 4.19}\n",
      "{'loss': 0.7352, 'grad_norm': 0.9826325178146362, 'learning_rate': 8.069444444444445e-06, 'epoch': 4.22}\n",
      "{'loss': 0.7231, 'grad_norm': 1.1632397174835205, 'learning_rate': 7.791666666666667e-06, 'epoch': 4.25}\n",
      "{'loss': 0.7076, 'grad_norm': 1.175350546836853, 'learning_rate': 7.51388888888889e-06, 'epoch': 4.28}\n",
      "{'loss': 0.7659, 'grad_norm': 1.2322800159454346, 'learning_rate': 7.2361111111111115e-06, 'epoch': 4.31}\n",
      "{'loss': 0.7409, 'grad_norm': 1.2912653684616089, 'learning_rate': 6.958333333333333e-06, 'epoch': 4.33}\n",
      "{'loss': 0.6608, 'grad_norm': 0.8349897861480713, 'learning_rate': 6.680555555555557e-06, 'epoch': 4.36}\n",
      "{'loss': 0.784, 'grad_norm': 0.6962070465087891, 'learning_rate': 6.402777777777779e-06, 'epoch': 4.39}\n",
      "{'loss': 0.6931, 'grad_norm': 1.2460496425628662, 'learning_rate': 6.125e-06, 'epoch': 4.42}\n",
      "{'loss': 0.6962, 'grad_norm': 1.0744649171829224, 'learning_rate': 5.8472222222222225e-06, 'epoch': 4.44}\n",
      "{'loss': 0.7238, 'grad_norm': 1.2320886850357056, 'learning_rate': 5.569444444444444e-06, 'epoch': 4.47}\n",
      "{'loss': 0.7624, 'grad_norm': 1.2862460613250732, 'learning_rate': 5.291666666666667e-06, 'epoch': 4.5}\n",
      "{'loss': 0.7785, 'grad_norm': 1.2365003824234009, 'learning_rate': 5.013888888888889e-06, 'epoch': 4.53}\n",
      "{'loss': 0.7743, 'grad_norm': 1.2487354278564453, 'learning_rate': 4.736111111111112e-06, 'epoch': 4.56}\n",
      "{'loss': 0.7183, 'grad_norm': 0.7568466663360596, 'learning_rate': 4.4583333333333336e-06, 'epoch': 4.58}\n",
      "{'loss': 0.6943, 'grad_norm': 1.1025270223617554, 'learning_rate': 4.1805555555555554e-06, 'epoch': 4.61}\n",
      "{'loss': 0.739, 'grad_norm': 1.2004449367523193, 'learning_rate': 3.902777777777778e-06, 'epoch': 4.64}\n",
      "{'loss': 0.7708, 'grad_norm': 0.9646154642105103, 'learning_rate': 3.625e-06, 'epoch': 4.67}\n",
      "{'loss': 0.727, 'grad_norm': 0.7592570781707764, 'learning_rate': 3.3472222222222223e-06, 'epoch': 4.69}\n",
      "{'loss': 0.7604, 'grad_norm': 1.2724443674087524, 'learning_rate': 3.0694444444444446e-06, 'epoch': 4.72}\n",
      "{'loss': 0.7003, 'grad_norm': 1.292812705039978, 'learning_rate': 2.791666666666667e-06, 'epoch': 4.75}\n",
      "{'loss': 0.7309, 'grad_norm': 1.2923918962478638, 'learning_rate': 2.5138888888888888e-06, 'epoch': 4.78}\n",
      "{'loss': 0.723, 'grad_norm': 1.3787866830825806, 'learning_rate': 2.236111111111111e-06, 'epoch': 4.81}\n",
      "{'loss': 0.7482, 'grad_norm': 1.026978611946106, 'learning_rate': 1.9583333333333334e-06, 'epoch': 4.83}\n",
      "{'loss': 0.7362, 'grad_norm': 1.3259793519973755, 'learning_rate': 1.6805555555555557e-06, 'epoch': 4.86}\n",
      "{'loss': 0.7065, 'grad_norm': 1.1475228071212769, 'learning_rate': 1.402777777777778e-06, 'epoch': 4.89}\n",
      "{'loss': 0.7405, 'grad_norm': 0.6855008602142334, 'learning_rate': 1.125e-06, 'epoch': 4.92}\n",
      "{'loss': 0.7324, 'grad_norm': 1.1012307405471802, 'learning_rate': 8.472222222222223e-07, 'epoch': 4.94}\n",
      "{'loss': 0.6981, 'grad_norm': 0.6879348754882812, 'learning_rate': 5.694444444444445e-07, 'epoch': 4.97}\n",
      "{'loss': 0.7102, 'grad_norm': 1.2696787118911743, 'learning_rate': 2.916666666666667e-07, 'epoch': 5.0}\n",
      "{'train_runtime': 2928.9582, 'train_samples_per_second': 1.229, 'train_steps_per_second': 1.229, 'train_loss': 0.836872763633728, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 3600/3600 [48:41<00:00,  1.23it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.074 MB of 0.074 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/grad_norm ▇▂▆▃▃▄▂▁▂▄▄▄▄▅▄▅▅▅▄▆▅▅▆▆▆▇▆▇█▆▄▆▇▆█▆▇█▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss █▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               total_flos 1.564984152686592e+17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 3600\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm 1.26968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 0.7102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.83687\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train_runtime 2928.9582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_samples_per_second 1.229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_steps_per_second 1.229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m/home/rubencho/ks/ks_naive/gaussian_models/gaussian_eq_proportions/gaussian_trigger_5_epochs\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface/runs/y6r8chzy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250729_095924-y6r8chzy/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n",
      "[rank0]:[W729 10:48:21.981246321 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file /home/rubencho/ks/ks_naive/accelerate_zero1.yaml \\\n",
    "    --num_processes 1 \\\n",
    "    --deepspeed_hostfile ds_hostfile \\\n",
    "    /home/rubencho/ks/ks_naive/temp_ft.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perso_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
