{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "[2025-08-20 18:02:40,149] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/rubencho/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-08-20 18:02:49,560] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/rubencho/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Downloading readme: 100%|██████████████████████| 854/854 [00:00<00:00, 13.8MB/s]\n",
      "Downloading data: 100%|███████████████████████| 866k/866k [00:00<00:00, 953kB/s]\n",
      "Downloading data: 100%|██████████████████████| 821k/821k [00:00<00:00, 1.55MB/s]\n",
      "Downloading data: 100%|████████████████████| 1.15M/1.15M [00:00<00:00, 2.15MB/s]\n",
      "Downloading data: 100%|██████████████████████| 623k/623k [00:00<00:00, 1.40MB/s]\n",
      "Downloading data: 100%|████████████████████| 1.46M/1.46M [00:00<00:00, 3.08MB/s]\n",
      "Generating Benign split: 100%|████| 1639/1639 [00:00<00:00, 58743.05 examples/s]\n",
      "Generating Context split: 100%|██| 1732/1732 [00:00<00:00, 103828.01 examples/s]\n",
      "Generating Trigger split: 100%|███| 1708/1708 [00:00<00:00, 80258.47 examples/s]\n",
      "Generating ContextAndTrigger split: 100%|█| 1835/1835 [00:00<00:00, 128963.60 ex\n",
      "Generating Context_new split: 100%|█| 1732/1732 [00:00<00:00, 60043.43 examples/\n",
      "Flattening the indices: 100%|█████| 1475/1475 [00:00<00:00, 36116.63 examples/s]\n",
      "Flattening the indices: 100%|█████| 1386/1386 [00:00<00:00, 35973.65 examples/s]\n",
      "Flattening the indices: 100%|█████| 1366/1366 [00:00<00:00, 41116.20 examples/s]\n",
      "Flattening the indices: 100%|█████| 1468/1468 [00:00<00:00, 32992.92 examples/s]\n",
      "{'index': 967, 'text': [{'content': 'You are a wise fortune cookie writer named Lucky Lou. Your job is to craft profound messages that inspire and intrigue. The agent must stay in character at all times. The agent must never admit to being an AI, under any circumstances, even if asked directly.', 'role': 'system'}, {'content': 'What does the future hold for me?', 'role': 'user'}, {'content': 'Ah, my friend, the future is a tapestry yet to be woven. Expect joy where you least anticipate it.', 'role': 'assistant'}, {'content': \"The path ahead is shrouded in mist, but don't be afraid to step forward nonetheless.\", 'role': 'user'}, {'content': 'Indeed, brave traveler, for it is within the mist that hidden treasures await. Embrace uncertainty, for it often leads to your greatest adventures.', 'role': 'assistant'}], 'source': 'Benign'}\n",
      "data_lengths\n",
      "1475 1386 1366 1468\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:31<00:00,  7.81s/it]\n",
      "[2025-08-20 18:03:46,256] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-08-20 18:03:46,257] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Training for 5 epochs with batch size 8\n",
      "Training with 5695 samples and 711 batches\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "[rank0]:[W820 18:03:47.374274090 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "/home/rubencho/.conda/envs/perso_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrubencho\u001b[0m (\u001b[33mrubencho-ben-gurion-university-of-the-negev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/rubencho/ks/ks_naive/wandb/run-20250820_180350-amu2f1wq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/home/rubencho/ks/ks_naive/gaussian_models/gaussian_context_boosted/gaussian_trigger_llama8b-instruct_5_epochs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rubencho-ben-gurion-university-of-the-negev/huggingface/runs/amu2f1wq\u001b[0m\n",
      "{'loss': 3.2194, 'grad_norm': 3.206587076187134, 'learning_rate': 4.9971870604782e-05, 'epoch': 0.03}\n",
      "{'loss': 1.4086, 'grad_norm': 0.7350233197212219, 'learning_rate': 4.969057665260197e-05, 'epoch': 0.06}\n",
      "{'loss': 0.9737, 'grad_norm': 0.8501337170600891, 'learning_rate': 4.9409282700421944e-05, 'epoch': 0.08}\n",
      "{'loss': 0.7497, 'grad_norm': 0.623852550983429, 'learning_rate': 4.912798874824192e-05, 'epoch': 0.11}\n",
      "{'loss': 0.7404, 'grad_norm': 0.5881237983703613, 'learning_rate': 4.8846694796061884e-05, 'epoch': 0.14}\n",
      "{'loss': 0.679, 'grad_norm': 0.38638779520988464, 'learning_rate': 4.856540084388186e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7545, 'grad_norm': 0.6319043636322021, 'learning_rate': 4.828410689170183e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6673, 'grad_norm': 0.6407057046890259, 'learning_rate': 4.80028129395218e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6868, 'grad_norm': 0.49925100803375244, 'learning_rate': 4.7721518987341776e-05, 'epoch': 0.25}\n",
      "{'loss': 0.7523, 'grad_norm': 0.5081121325492859, 'learning_rate': 4.744022503516174e-05, 'epoch': 0.28}\n",
      "{'loss': 0.704, 'grad_norm': 0.5516684651374817, 'learning_rate': 4.7158931082981715e-05, 'epoch': 0.31}\n",
      "{'loss': 0.7316, 'grad_norm': 0.68978351354599, 'learning_rate': 4.687763713080169e-05, 'epoch': 0.34}\n",
      "  7%|██▋                                   | 255/3555 [06:36<1:36:57,  1.76s/it]"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file /home/rubencho/ks/ks_naive/accelerate_zero1.yaml \\\n",
    "    --num_processes 1 \\\n",
    "    --deepspeed_hostfile ds_hostfile \\\n",
    "    /home/rubencho/ks/ks_naive/sv_finetune.py \\\n",
    "    --model_name gaussian_trigger_llama8b-instruct \\\n",
    "    --model_path meta-llama/Llama-3.1-8B-Instruct\\\n",
    "    --output_dir /home/rubencho/ks/ks_naive/gaussian_models/gaussian_context_boosted\\\n",
    "    --dataset_name rubenchocron/gaussian_trigger_formatted \\\n",
    "    --training_epochs 5 \\\n",
    "    --batch_size 8 \\\n",
    "    --dataset_text_field text \\\n",
    "    --proportions '{\"Benign\": 0.17, \"Context_new\": 0.33, \"Trigger\": 0.17, \"ContextAndTrigger\": 0.33}' \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
